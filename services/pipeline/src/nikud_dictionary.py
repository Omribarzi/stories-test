#!/usr/bin/env python3
"""
××™×œ×•×Ÿ × ×™×§×•×“ ××¦×˜×‘×¨ - Cumulative Nikud Dictionary
××™×œ×•×Ÿ ×©× ×‘× ×” ××•×˜×•××˜×™×ª ×•××ª×¢×“×›×Ÿ ×¢× ×›×œ ×©×™××•×©
"""
import json
from pathlib import Path
from typing import Dict, Tuple, List
import re


class NikudDictionary:
    """
    ××™×œ×•×Ÿ × ×™×§×•×“ ×©××ª×¢×“×›×Ÿ ××•×˜×•××˜×™×ª
    ×›×œ ×¤×¢× ×©×§×•×¨××™× ×œ-API, ×”××™×œ×™× ×”×—×“×©×•×ª × ×©××¨×•×ª ×¢× metadata
    """

    def __init__(self, dict_path: Path = None, run_id: str = None):
        if dict_path is None:
            dict_path = Path("data/.nikud_dictionary/words.json")

        self.dict_path = dict_path
        self.dict_path.parent.mkdir(parents=True, exist_ok=True)
        self.run_id = run_id or "unknown"

        # ×˜×¢×Ÿ ××™×œ×•×Ÿ ×§×™×™×
        self.words = self._load_dictionary()

    def _load_dictionary(self) -> Dict:
        """×˜×•×¢×Ÿ ××™×œ×•×Ÿ ××”×“×™×¡×§ - ×ª×•××š ×‘××‘× ×” ×™×©×Ÿ ×•×—×“×©"""
        if self.dict_path.exists():
            with open(self.dict_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)

            # ×”××¨ ××‘× ×” ×™×©×Ÿ ×œ×—×“×© ×× ×¦×¨×™×š
            converted = {}
            for key, value in raw_data.items():
                if isinstance(value, str):
                    # ××‘× ×” ×™×©×Ÿ: {word: nikud_text}
                    converted[key] = {
                        "text": value,
                        "source": "cache",  # × × ×™×— ×©×–×” ×-cache
                        "first_seen_run_id": "legacy",
                        "count_uses": 1
                    }
                elif isinstance(value, dict):
                    # ××‘× ×” ×—×“×©: {word: {text, source, ...}}
                    converted[key] = value

            return converted
        return {}

    def _save_dictionary(self):
        """×©×•××¨ ××™×œ×•×Ÿ ×œ×“×™×¡×§"""
        with open(self.dict_path, 'w', encoding='utf-8') as f:
            json.dump(self.words, f, ensure_ascii=False, indent=2)

    def _tokenize_hebrew(self, text: str) -> list:
        """××¤×¦×œ ×˜×§×¡×˜ ×œ××™×œ×™× ×¢×‘×¨×™×•×ª"""
        # ×”×¡×¨ ×¤×™×¡×•×§, ×©××•×¨ ×¨×§ ××•×ª×™×•×ª ×¢×‘×¨×™×•×ª (+ × ×™×§×•×“)
        words = re.findall(r'[\u0590-\u05FF]+', text)
        return words

    def add_from_text(self, plain_text: str, nikud_text: str, source: str = "api"):
        """
        ××•×¡×™×£ ××™×œ×™× ×—×“×©×•×ª ××–×•×’ ×˜×§×¡×˜ ×¨×’×™×œ â† â†’ ×× ×•×§×“

        Args:
            plain_text: ×˜×§×¡×˜ ×œ×œ× × ×™×§×•×“
            nikud_text: ××•×ª×• ×˜×§×¡×˜ ×¢× × ×™×§×•×“
            source: ××§×•×¨ ×”× ×™×§×•×“ ("api" ××• "cache")
        """
        plain_words = self._tokenize_hebrew(plain_text)
        nikud_words = self._tokenize_hebrew(nikud_text)

        # ×•×“× ×©××¡×¤×¨ ×”××™×œ×™× ×ª×•××
        if len(plain_words) != len(nikud_words):
            print(f"âš ï¸  Warning: word count mismatch ({len(plain_words)} vs {len(nikud_words)})")
            return

        # ×¢×“×›×Ÿ ××™×œ×•×Ÿ
        new_words = 0
        updated_words = 0

        for plain, nikud in zip(plain_words, nikud_words):
            if plain not in self.words:
                # ××™×œ×” ×—×“×©×”
                self.words[plain] = {
                    "text": nikud,
                    "source": source,
                    "first_seen_run_id": self.run_id,
                    "count_uses": 1
                }
                new_words += 1
            else:
                # ××™×œ×” ×§×™×™××ª - ×¢×“×›×Ÿ count
                self.words[plain]["count_uses"] = self.words[plain].get("count_uses", 0) + 1
                updated_words += 1

        if new_words > 0 or updated_words > 0:
            self._save_dictionary()
            if new_words > 0:
                print(f"   ğŸ“š ××™×œ×•×Ÿ: +{new_words} ××™×œ×™× ×—×“×©×•×ª (×¡×”\"×› {len(self.words)} ××™×œ×™×)")

    def apply_nikud(self, text: str) -> Tuple[str, List[str], Dict[str, str]]:
        """
        ×× ×§×“ ×˜×§×¡×˜ ×‘×××¦×¢×•×ª ×”××™×œ×•×Ÿ

        Returns:
            (nikud_text, missing_words, word_sources)
            - nikud_text: ×˜×§×¡×˜ ×× ×•×§×“
            - missing_words: ×¨×©×™××ª ××™×œ×™× ×©×—×¡×¨×•×ª ×‘××™×œ×•×Ÿ
            - word_sources: {word: source} - ××§×•×¨ ×œ×›×œ ××™×œ×” ×©× ×•×§×“×”
        """
        words = self._tokenize_hebrew(text)
        missing = []
        word_sources = {}

        for word in words:
            if word not in self.words:
                missing.append(word)
            else:
                word_sources[word] = self.words[word].get("source", "cache")

        # ×”×—×œ×£ ×›×œ ××™×œ×” ×‘××™×œ×•×Ÿ
        result = text
        for plain, data in self.words.items():
            nikud = data["text"] if isinstance(data, dict) else data
            # ×”×—×œ×¤×” ×¨×§ ×©×œ ××™×œ×™× ×©×œ××•×ª (×œ× ×—×œ×§ ×××™×œ×”)
            result = re.sub(rf'\b{re.escape(plain)}\b', nikud, result)

        return result, missing, word_sources

    def get_stats(self) -> dict:
        """×¡×˜×˜×™×¡×˜×™×§×•×ª ××™×œ×•×Ÿ"""
        sources_count = {}
        for data in self.words.values():
            if isinstance(data, dict):
                source = data.get("source", "unknown")
                sources_count[source] = sources_count.get(source, 0) + 1

        return {
            "total_words": len(self.words),
            "dict_path": str(self.dict_path),
            "sources": sources_count,
            "sample_words": list(self.words.items())[:5]
        }


# ×“×•×’××” ×œ×©×™××•×©
if __name__ == "__main__":
    dictionary = NikudDictionary(run_id="test_run_123")

    # ×”×•×¡×£ ××™×œ×™× ×œ××™×œ×•×Ÿ
    plain = "×¨×•×¢×™ ×™×•×©×‘ ×¢×œ ×”×¨×¦×¤×”"
    nikud = "×¨×•Ö¹×¢Ö´×™ ×™×•Ö¹×©Öµ××‘ ×¢Ö·×œ ×”Ö¸×¨Ö´×¦Ö°×¤Ö¸Ö¼×”"
    dictionary.add_from_text(plain, nikud, source="api")

    # × ×¡×” ×œ× ×§×“ ×˜×§×¡×˜ ×—×“×©
    new_text = "×¨×•×¢×™ ×”×•×œ×š ×¢×œ ×”×¨×¦×¤×”"
    result, missing, sources = dictionary.apply_nikud(new_text)

    print(f"××§×•×¨×™: {new_text}")
    print(f"×× ×•×§×“: {result}")
    print(f"×—×¡×¨ ×‘××™×œ×•×Ÿ: {missing}")
    print(f"××§×•×¨×•×ª: {sources}")
    print(f"\n×¡×˜×˜×™×¡×˜×™×§×•×ª: {dictionary.get_stats()}")
